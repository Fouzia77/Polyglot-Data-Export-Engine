
# ğŸ“¦ Polyglot Data Export Engine

A high-performance, Dockerized data export service built with **Node.js (Fastify)** and **PostgreSQL** that streams large datasets into multiple formats (CSV, JSON, XML, Parquet) with optional Gzip compression.

---

## ğŸš€ Features

* âš¡ Fast streaming exports (memory efficient)
* ğŸ³ Fully Dockerized setup
* ğŸ—„ PostgreSQL backend
* ğŸ“„ Multiple export formats:

  * CSV
  * JSON
  * XML
  * Parquet
* ğŸ—œ Optional Gzip compression
* ğŸ“Š Benchmark endpoint
* ğŸ”— REST API with Fastify

---

## ğŸ— Architecture

```id="wq6mbp"
Client (Postman/Browser)
        â†“
   Fastify API (Node.js)
        â†“
   PostgreSQL Database
        â†“
 Streaming Writers (CSV/JSON/XML/Parquet)
        â†“
     HTTP Response (Stream)
```

---

## ğŸ“ Project Structure

```id="of9fch"
Polyglot Data Export Engine/
â”‚
â”œâ”€â”€ source_code/
â”‚   â”œâ”€â”€ server.js
â”‚   â”œâ”€â”€ db.js
â”‚   â”œâ”€â”€ exportService.js
â”‚   â”œâ”€â”€ benchmark.js
â”‚   â””â”€â”€ writers/
â”‚       â”œâ”€â”€ csvWriter.js
â”‚       â”œâ”€â”€ jsonWriter.js
â”‚       â”œâ”€â”€ xmlWriter.js
â”‚       â””â”€â”€ parquetWriter.js
â”‚
â”œâ”€â”€ init-db.sh
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ package.json
```

---

## ğŸ³ Running with Docker

### 1ï¸âƒ£ Build & Start

```bash id="5xlp82"
docker compose up --build
```

App will run at:

```id="7pq2ra"
http://localhost:8080
```

---

### 2ï¸âƒ£ Stop Containers

```bash id="kj1wqf"
docker compose down
```

---

## ğŸ—„ Database

PostgreSQL runs inside Docker.

### Default Credentials

```id="6uub8e"
Host: db
Port: 5432
User: user
Password: password
Database: exports_db
```

---

## ğŸ“¡ API Endpoints

### ğŸ”¹ 1. Create Export Job

**POST** `/exports`

#### Request Body

```json id="7ol0d1"
{
  "format": "csv",
  "columns": ["id", "name", "value"],
  "compression": "gzip"
}
```

#### Response

```json id="1esqq5"
{
  "exportId": "uuid",
  "status": "pending"
}
```

---

### ğŸ”¹ 2. Download Export

**GET** `/exports/:id/download`

Example:

```id="qz2uul"
GET http://localhost:8080/exports/<exportId>/download
```

ğŸ“¥ Downloads file in chosen format.

---

### ğŸ”¹ 3. Benchmark

**GET** `/exports/benchmark`

Returns performance metrics.

---

## ğŸ“„ Supported Formats

| Format  | Content-Type                   | Compression |
| ------- | ------------------------------ | ----------- |
| CSV     | text/csv                       | âœ… gzip      |
| JSON    | application/json               | âœ… gzip      |
| XML     | application/xml                | âœ… gzip      |
| Parquet | application/vnd.apache.parquet | âŒ           |

---

## ğŸ§ª Testing with Postman

### Step 1 â€” Create Export

POST â†’ `http://localhost:8080/exports`

### Step 2 â€” Download File

GET â†’

```id="b8s6t9"
http://localhost:8080/exports/<exportId>/download
```

---

## âš™ï¸ Environment Variables

```id="d40j9c"
PORT=8080
DATABASE_URL=postgres://user:password@db:5432/exports_db
```

---

## ğŸ§° Tech Stack

* Node.js 20
* Fastify
* PostgreSQL 13
* Docker & Docker Compose
* Streams API
* ParquetJS

---

## ğŸ“Œ How It Works

1. User requests export via API
2. Job stored in memory
3. Writer streams data from PostgreSQL
4. Data transformed to requested format
5. Optional gzip compression
6. Stream sent to client

---

## ğŸ›  Troubleshooting

### âŒ ECONNREFUSED (DB connection)

âœ” Ensure containers are running:

```bash id="hmh0rq"
docker ps
```

---

### âŒ Table not found

Rebuild DB volume:

```bash id="2z9r0l"
docker compose down -v
docker compose up --build
```








